{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将视频拆成60张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def video_to_frames(video_path, output_path, frame_interval=1):\n",
    "    # 打开视频文件\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # 按照指定帧间隔保存图像\n",
    "        if count % frame_interval == 0:\n",
    "            frame_output_path = f\"{output_path}/frame_{count}.jpg\"\n",
    "            cv2.imwrite(frame_output_path, frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "\n",
    "# 视频文件路径\n",
    "video_path = r'I:\\mydesk\\bishi\\data\\trimmed_video.mp4'\n",
    "# 输出图像路径\n",
    "output_path = r'I:\\mydesk\\pytorch_face_landmark-master\\new_samples\\12--Group'\n",
    "# 保存图像的帧间隔\n",
    "frame_interval = 1  # 每1帧保存一次图像\n",
    "\n",
    "video_to_frames(video_path, output_path, frame_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 识别图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from common.utils import BBox,drawLandmark,drawLandmark_multiple\n",
    "from models.basenet import MobileNet_GDConv\n",
    "from models.pfld_compressed import PFLDInference\n",
    "from models.mobilefacenet import MobileFaceNet\n",
    "from FaceBoxes import FaceBoxes\n",
    "from Retinaface import Retinaface\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from MTCNN import detect_faces\n",
    "import glob\n",
    "import time\n",
    "from utils.align_trans import get_reference_facial_points, warp_and_crop_face\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch face landmark')\n",
    "# Datasets\n",
    "parser.add_argument('--backbone', default='MobileFaceNet', type=str,\n",
    "                    help='choose which backbone network to use: MobileNet, PFLD, MobileFaceNet')\n",
    "parser.add_argument('--detector', default='Retinaface', type=str,\n",
    "                    help='choose which face detector to use: MTCNN, FaceBoxes, Retinaface')\n",
    "\n",
    "args = parser.parse_args()\n",
    "mean = np.asarray([ 0.485, 0.456, 0.406 ])\n",
    "std = np.asarray([ 0.229, 0.224, 0.225 ])\n",
    "\n",
    "crop_size= 112\n",
    "scale = crop_size / 112.\n",
    "reference = get_reference_facial_points(default_square = True) * scale\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'\n",
    "\n",
    "def load_model():\n",
    "    if args.backbone=='MobileNet':\n",
    "        model = MobileNet_GDConv(136)\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        # download model from https://drive.google.com/file/d/1Le5UdpMkKOTRr1sTp4lwkw8263sbgdSe/view?usp=sharing\n",
    "        checkpoint = torch.load('checkpoint/mobilenet_224_model_best_gdconv_external.pth.tar', map_location=map_location)\n",
    "        print('Use MobileNet as backbone')\n",
    "    elif args.backbone=='PFLD':\n",
    "        model = PFLDInference() \n",
    "        # download from https://drive.google.com/file/d/1gjgtm6qaBQJ_EY7lQfQj3EuMJCVg9lVu/view?usp=sharing\n",
    "        checkpoint = torch.load('checkpoint/pfld_model_best.pth.tar', map_location=map_location)\n",
    "        print('Use PFLD as backbone') \n",
    "        # download from https://drive.google.com/file/d/1T8J73UTcB25BEJ_ObAJczCkyGKW5VaeY/view?usp=sharing\n",
    "    elif args.backbone=='MobileFaceNet':\n",
    "        model = MobileFaceNet([112, 112],136)   \n",
    "        checkpoint = torch.load('checkpoint/mobilefacenet_model_best.pth.tar', map_location=map_location)      \n",
    "        print('Use MobileFaceNet as backbone')         \n",
    "    else:\n",
    "        print('Error: not suppored backbone')    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if args.backbone=='MobileNet':\n",
    "        out_size = 224\n",
    "    else:\n",
    "        out_size = 112 \n",
    "    model = load_model()\n",
    "    model = model.eval()\n",
    "    filenames=glob.glob(\"samples/12--Group/*.jpg\")\n",
    "    for imgname in filenames:\n",
    "        print(imgname)\n",
    "        img = cv2.imread(imgname)\n",
    "        org_img = Image.open(imgname)\n",
    "        height,width,_=img.shape\n",
    "        if args.detector=='MTCNN':\n",
    "            # perform face detection using MTCNN\n",
    "            image = Image.open(imgname)\n",
    "            faces, landmarks = detect_faces(image)\n",
    "        elif args.detector=='FaceBoxes':\n",
    "            face_boxes = FaceBoxes()\n",
    "            faces = face_boxes(img)\n",
    "        elif args.detector=='Retinaface':\n",
    "            retinaface=Retinaface.Retinaface()    \n",
    "            faces = retinaface(img)            \n",
    "        else:\n",
    "            print('Error: not suppored detector')        \n",
    "        ratio=0\n",
    "        if len(faces)==0:\n",
    "            print('NO face is detected!')\n",
    "            continue\n",
    "        for k, face in enumerate(faces): \n",
    "            if face[4]<0.9: # remove low confidence detection\n",
    "                continue\n",
    "            x1=face[0]\n",
    "            y1=face[1]\n",
    "            x2=face[2]\n",
    "            y2=face[3]\n",
    "            w = x2 - x1 + 1\n",
    "            h = y2 - y1 + 1\n",
    "            size = int(min([w, h])*1.2)\n",
    "            cx = x1 + w//2\n",
    "            cy = y1 + h//2\n",
    "            x1 = cx - size//2\n",
    "            x2 = x1 + size\n",
    "            y1 = cy - size//2\n",
    "            y2 = y1 + size\n",
    "\n",
    "            dx = max(0, -x1)\n",
    "            dy = max(0, -y1)\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "\n",
    "            edx = max(0, x2 - width)\n",
    "            edy = max(0, y2 - height)\n",
    "            x2 = min(width, x2)\n",
    "            y2 = min(height, y2)\n",
    "            new_bbox = list(map(int, [x1, x2, y1, y2]))\n",
    "            new_bbox = BBox(new_bbox)\n",
    "            cropped=img[new_bbox.top:new_bbox.bottom,new_bbox.left:new_bbox.right]\n",
    "            if (dx > 0 or dy > 0 or edx > 0 or edy > 0):\n",
    "                cropped = cv2.copyMakeBorder(cropped, int(dy), int(edy), int(dx), int(edx), cv2.BORDER_CONSTANT, 0)            \n",
    "            cropped_face = cv2.resize(cropped, (out_size, out_size))\n",
    "\n",
    "            if cropped_face.shape[0]<=0 or cropped_face.shape[1]<=0:\n",
    "                continue\n",
    "            test_face = cropped_face.copy()\n",
    "            test_face = test_face/255.0\n",
    "            if args.backbone=='MobileNet':\n",
    "                test_face = (test_face-mean)/std\n",
    "            test_face = test_face.transpose((2, 0, 1))\n",
    "            test_face = test_face.reshape((1,) + test_face.shape)\n",
    "            input = torch.from_numpy(test_face).float()\n",
    "            input= torch.autograd.Variable(input)\n",
    "            start = time.time()\n",
    "            if args.backbone=='MobileFaceNet':\n",
    "                landmark = model(input)[0].cpu().data.numpy()\n",
    "            else:\n",
    "                landmark = model(input).cpu().data.numpy()\n",
    "            end = time.time()\n",
    "            print('Time: {:.6f}s.'.format(end - start))\n",
    "            landmark = landmark.reshape(-1,2)\n",
    "            landmark = new_bbox.reprojectLandmark(landmark)\n",
    "            img = drawLandmark_multiple(img, new_bbox, landmark)\n",
    "            # crop and aligned the face\n",
    "            lefteye_x=0\n",
    "            lefteye_y=0\n",
    "            for i in range(36,42):\n",
    "                lefteye_x+=landmark[i][0]\n",
    "                lefteye_y+=landmark[i][1]\n",
    "            lefteye_x=lefteye_x/6\n",
    "            lefteye_y=lefteye_y/6\n",
    "            lefteye=[lefteye_x,lefteye_y]\n",
    "\n",
    "            righteye_x=0\n",
    "            righteye_y=0\n",
    "            for i in range(42,48):\n",
    "                righteye_x+=landmark[i][0]\n",
    "                righteye_y+=landmark[i][1]\n",
    "            righteye_x=righteye_x/6\n",
    "            righteye_y=righteye_y/6\n",
    "            righteye=[righteye_x,righteye_y]  \n",
    "\n",
    "            nose=landmark[33]\n",
    "            leftmouth=landmark[48]\n",
    "            rightmouth=landmark[54]\n",
    "            facial5points=[righteye,lefteye,nose,rightmouth,leftmouth]\n",
    "            warped_face = warp_and_crop_face(np.array(org_img), facial5points, reference, crop_size=(crop_size, crop_size))\n",
    "            img_warped = Image.fromarray(warped_face)\n",
    "            # save the aligned and cropped faces\n",
    "            img_warped.save(os.path.join('results_aligned', os.path.basename(imgname)[:-4]+'_'+str(k)+'.png'))  \n",
    "            #img = drawLandmark_multiple(img, new_bbox, facial5points)  # plot and show 5 points   \n",
    "        # save the landmark detections \n",
    "        cv2.imwrite(os.path.join('results',os.path.basename(imgname)),img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将识别出来的图片合成视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptorchcopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
